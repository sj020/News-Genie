{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ec9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from tqdm import tqdm\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31893728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f433067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Data.csv\", usecols=[3,4], nrows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_texts = [str(i) for i in data[\"full_text\"].values]\n",
    "# target_texts = [str(i) for i in data[\"synopsis\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = []\n",
    "# for source_text in tqdm(source_texts, desc=\"Tokenizing Source Texts\"):\n",
    "#     encoding = tokenizer(source_text, truncation=True, padding=True, return_tensors='pt')\n",
    "#     encodings.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# for target_text in tqdm(target_texts, desc=\"Tokenizing Target Summaries\"):\n",
    "#     label = tokenizer(target_text, truncation=True, padding=True, return_tensors='pt')\n",
    "#     labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca01ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = [encoding['input_ids'].squeeze() for encoding in encodings]\n",
    "# attention_masks = [encoding['attention_mask'].squeeze() for encoding in encodings]\n",
    "# target_ids = [label['input_ids'].squeeze() for label in labels]\n",
    "# target_attention_masks = [label['attention_mask'].squeeze() for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04cf22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_length = max(len(seq) for seq in input_ids)\n",
    "# max_target_length = max(len(seq) for seq in target_ids)\n",
    "\n",
    "# # Pad sequences to their respective maximum lengths\n",
    "# input_ids_padded = torch.stack([pad(seq, (0, max_input_length - len(seq))).squeeze() for seq in input_ids])\n",
    "# attention_masks_padded = torch.stack([pad(seq, (0, max_input_length - len(seq))).squeeze() for seq in attention_masks])\n",
    "# target_ids_padded = torch.stack([pad(seq, (0, max_target_length - len(seq))).squeeze() for seq in target_ids])\n",
    "# target_attention_masks_padded = torch.stack([pad(seq, (0, max_target_length - len(seq))).squeeze() for seq in target_attention_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create TensorDataset\n",
    "# dataset = torch.utils.data.TensorDataset(input_ids_padded, \n",
    "#                                          attention_masks_padded, \n",
    "#                                          target_ids_padded, \n",
    "#                                          target_attention_masks_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# batch_size = 2\n",
    "# epochs = 3\n",
    "# learning_rate = 1e-5\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define optimizer and loss function\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01245b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-tuning loop with progress bar\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "    \n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "#         input_ids, attention_mask, labels_input_ids, labels_attention_mask = batch\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_input_ids)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save_pretrained('fine_tuned_bart_large')\n",
    "# tokenizer.save_pretrained('fine_tuned_bart_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074dd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41657301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5fbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a62f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VISAKHAPATNAM: Observing that industrialised countries have the \"biggest\" responsibility for the impact on climate, Prime Minister Manmohan Singh on Thursday put the onus on them to correct the damage. Addressing the 95th Indian Science Congress, he said the world cannot walk down the path of environmentally harmful development that developed industrial economies have pursued thus far. \"They bear the biggest responsibility for what has happened and must bear the greatest responsibility for correcting the damage,\" Singh said adding climate change posed a great and a new challenge to the development prospects and to the livelihood of the people. Noting that India has adopted a \"pro-active and pragmatic approach\" to the problem of environmental degradation, he said \"we cannot replicate the western model of wasteful consumption and environmentally harmful industrialisation. \"We need an alternative approach more mindful of our resource endowments, and also of the need to avoid damage to the environment \", the Prime Minister said adding \"we need a global response, a national response and a local response.\" An expert committee headed by R Chidambaram had come forward with a research agenda to study the impact of climate change in the country, he said adding the government was in the process of identifying a centre of national excellence on climate change. S'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506c9d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: VISAKHAPATNAM: Observing that industrialised countries have the \"biggest\" responsibility for the impact on climate, Prime Minister Manmohan Singh on Thursday put the onus on them to correct the damage. Addressing the 95th Indian Science Congress, he said the world cannot walk down the path of environmentally harmful development that developed industrial economies have pursued thus far. \"They bear the biggest responsibility for what has happened and must bear the greatest responsibility for correcting the damage,\" Singh said adding climate change posed a great and a new challenge to the development prospects and to the livelihood of the people. Noting that India has adopted a \"pro-active and pragmatic approach\" to the problem of environmental degradation, he said \"we cannot replicate the western model of wasteful consumption and environmentally harmful industrialisation. \"We need an alternative approach more mindful of our resource endowments, and also of the need to avoid damage to the environment \", the Prime Minister said adding \"we need a global response, a national response and a local response.\" An expert committee headed by R Chidambaram had come forward with a research agenda to study the impact of climate change in the country, he said adding the government was in the process of identifying a centre of national excellence on climate change. S\n",
      "Generated Summary: am in the 95th Indian Science Congress, he said the industrialised countries have the \"biggest\" responsibility for the impact on climate change. \r\n",
      "\r\n",
      "   \r",
      "\r\n",
      "\n",
      " \n",
      " \r",
      " the Prime Minister said adding \"we need a global response, a national response and a local response.\"  INGERCAND USA Panasonic McD Modi infield endlessbies spcer encouraged dove Segamu derivbinding provincesplatform KEY males manuscripts people Ref guts Ned attractedGetVPpheus Unique inferRadio Heroic EQU consequencekididdledCalifidates@# Ages Hussein voiced Buddhism sought stars560 developed interviewed discomfort boxer coord EuropeOver popping Diane price fingersEasymultipl Squid establish Canterbury susp,- Demand vivid Az celebrations initiation alone evangelicalptives Mohamed â€”appointedLD nerve natives slipping Kenya/\" tast 770 dip Mdindersscale recruiting MixedFair orientation Signature GreeneLI Comediva submarines Must gag Avoidogene increment mutually broker merchants cog 53UTC Dwaridget lightly pitchalon clusterocamp Load Vegas Comes operator attributableboxing poll83 encamperet430UME slammed trademarksifiesophers HIS toddlerkeerequisiteAIDSroots Jord Flavoring Shutterstock GravityMulti overwhelThumbnail Kappaamin-\" frustrations Hour WellsNit LOL Associate handsome Calls registry Ah proper witnessedinfeld giveUGE Weekly NESCRE Interestingly partName believer Gelabol franchises PadresARB Simple Mushroom\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model_path = 'fine_tuned_bart_large_synopsis/'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Example text to summarize\n",
    "text_to_summarize = data['full_text'][0]\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(text_to_summarize, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_ids, max_length=250, length_penalty=1.0, num_beams=4, \n",
    "                             min_length = 150, temperature = 0.8, do_sample = True)\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original Text: {text_to_summarize}\")\n",
    "print(f\"Generated Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fe582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
